{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries And Reading All Files From Drive**"
      ],
      "metadata": {
        "id": "pWCvcF1Dgw1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "iuMdeFPRI7QA"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the Excel file using pandas:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#please keep all the files in a folder in google drive named\"test\" before running the following program\n",
        "file_path = '/content/drive/MyDrive/test/Input.xlsx'\n",
        "#create a dataframe  of data in excel file\n",
        "df1 = pd.read_excel(file_path)\n",
        "print(df1)\n"
      ],
      "metadata": {
        "id": "Eoyop5YrJCNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check All the folders and files in test folder\n",
        "print(os.listdir('/content/drive/MyDrive/test/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpHGHneBNx31",
        "outputId": "e33715f4-0ae1-4229-ac56-308d562b8cf3"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Input.xlsx', 'Text Analysis.docx', 'Objective.docx', 'StopWords', 'MasterDictionary', 'Output', 'CleanedOutput', 'Output Data Structure.xlsx', 'FinalOutput.xlsx']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#allocating all the paths\n",
        "\n",
        "\n",
        "# Ensure output directory exists in google drive\n",
        "output_folder = '/content/drive/MyDrive/test/Output/'\n",
        "# If output directory does't exists in google drive it will create a new directory\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "     # Create a folder in the root directory\n",
        "    !mkdir -p \"/content/drive/My Drive/test/Output/\"\n",
        "\n",
        "\n",
        "\n",
        "stopwords_folder_path = '/content/drive/MyDrive/test/StopWords/'\n",
        "cleaned_files_folder_path = '/content/drive/MyDrive/test/CleanedOutput/'\n",
        "\n",
        "# Ensure the cleaned_files folder exists\n",
        "if not os.path.exists(cleaned_files_folder_path):\n",
        "    os.makedirs(cleaned_files_folder_path)\n",
        "        # Create a folder in the root directory\n",
        "    !mkdir -p \"/content/drive/My Drive/test/CleanedOutput/\"\n",
        "\n",
        "Master_Dictionary = '/content/drive/MyDrive/test/MasterDictionary/'\n",
        "stopwords_folder_path = os.path.join(Master_Dictionary, '/content/drive/MyDrive/test/StopWords/')\n",
        "positive_file_path = os.path.join(Master_Dictionary, 'positive-words.txt')\n",
        "negative_file_path = os.path.join(Master_Dictionary, 'negative-words.txt')\n",
        "Output_Data_Structure = '/content/drive/MyDrive/test/Output Data Structure.xlsx'\n"
      ],
      "metadata": {
        "id": "brwQjzo3AOL5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracting Articles From Given URL And Save Them In Output Folder In Drive**"
      ],
      "metadata": {
        "id": "oHK9tYMRhPsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Loop through each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    url_id_str = str(row['URL_ID']).lstrip(\"\\\\\") #\"URL_ID\" is the coloumn name in excel file\n",
        "    if url_id_str.endswith('.0'):\n",
        "        url_id_str = str(int(float(url_id_str)))\n",
        "\n",
        "    url_link = row['URL']  #\"URL\" is the coloumn name in excel file\n",
        "\n",
        "    # Fetch the webpage content\n",
        "    response = requests.get(url_link)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract title and content\n",
        "    #Parse the content using BeautifulSoup to extract the title and article text\n",
        "    # Assuming the webpage has title inside <title> tag and article text inside <p> tags\n",
        "    title = soup.title.string if soup.title else \"No Title\"\n",
        "    article_texts = soup.find_all('p')\n",
        "    article_content = '\\n'.join([text.get_text() for text in article_texts])\n",
        "\n",
        "    # Save to a text file in the outpur folder\n",
        "    file_path = os.path.join(output_folder, f\"{url_id_str}.txt\")\n",
        "    print(f\"Saving to {file_path}\")  # Print the path where we're saving to confirm\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(title + '\\n' + article_content)"
      ],
      "metadata": {
        "id": "0yfOMcRKJE6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning using Stop Words Lists And Saving Cleaned Files In A Folder \"CleanedOutput\"**"
      ],
      "metadata": {
        "id": "_xof1CmngcRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning using Stop Words Lists\n",
        "\n",
        "\n",
        "# List all stopwords and output files\n",
        "stopwords_files = [f for f in os.listdir(stopwords_folder_path) if os.path.isfile(os.path.join(stopwords_folder_path, f))]\n",
        "output_files = [f for f in os.listdir(output_folder) if os.path.isfile(os.path.join(output_folder, f))]\n",
        "\n",
        "# Dictionary to store counters\n",
        "counters = {}\n",
        "\n",
        "# For each file in the output folder\n",
        "for output_file in output_files:\n",
        "    content_file_path = os.path.join(output_folder, output_file)\n",
        "\n",
        "    with open(content_file_path, 'r', encoding='ISO-8859-1', errors='ignore') as f:\n",
        "        content = f.read().split()  # Splitting into words for easier counting\n",
        "\n",
        "    # Iteratively clean the content using each stopwords file\n",
        "    for stopwords_file in stopwords_files:\n",
        "        with open(os.path.join(stopwords_folder_path, stopwords_file), 'r', encoding='ISO-8859-1', errors='ignore') as f:\n",
        "            stopwords = set(f.read().splitlines())\n",
        "\n",
        "        # Count occurrences of each stopword in the content\n",
        "        found_stopwords = sum(1 for word in content if word.lower() in stopwords)\n",
        "        counters[(output_file, stopwords_file)] = found_stopwords\n",
        "\n",
        "        content = [word for word in content if word.lower() not in stopwords]\n",
        "\n",
        "    # Save the final cleaned content to the cleaned_files folder\n",
        "    cleaned_content_file_path = os.path.join(cleaned_files_folder_path, output_file)\n",
        "    with open(cleaned_content_file_path, 'w', encoding='ISO-8859-1') as f:\n",
        "        f.write(' '.join(content))\n",
        "\n",
        "# Print the summary\n",
        "for (output_file, stopwords_file), count in counters.items():\n",
        "    print(f\"In {output_file}, found {count} stopwords from {stopwords_file} and they have been cleaned.\")\n",
        "\n",
        "print(\"All files processed.\")\n"
      ],
      "metadata": {
        "id": "Be7-RQvhfZ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Dictionary Of Positive And Negative Words**"
      ],
      "metadata": {
        "id": "Kygz7OZN0bBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to your files\n",
        "\n",
        "\n",
        "\n",
        "# Function to read words from a file into a list\n",
        "def load_words_from_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        words = file.read().splitlines()\n",
        "    return words\n",
        "\n",
        "# Load all stopwords from every file in the stopwords folder into a single set\n",
        "stopwords_files = [f for f in os.listdir(stopwords_folder_path) if os.path.isfile(os.path.join(stopwords_folder_path, f))]\n",
        "stopwords = set()\n",
        "for stopwords_file in stopwords_files:\n",
        "    stopwords.update(load_words_from_file(os.path.join(stopwords_folder_path, stopwords_file)))\n",
        "\n",
        "# Load positive and negative words, filtering out stopwords\n",
        "positive_words = [word for word in load_words_from_file(positive_file_path) if word.lower() not in stopwords]\n",
        "negative_words = [word for word in load_words_from_file(negative_file_path) if word.lower() not in stopwords]\n",
        "\n",
        "# Combine into a dictionary\n",
        "words_dict = {\n",
        "    'positive': positive_words,\n",
        "    'negative': negative_words\n",
        "}\n",
        "\n",
        "# Optional: Print the dictionary to see the result\n",
        "print(words_dict)\n"
      ],
      "metadata": {
        "id": "GPeHzuS-JEtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracting Derived variables**"
      ],
      "metadata": {
        "id": "APK3HDUs216i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " nltk.download('punkt')\n",
        "\n",
        "# Data dictionary to store the scores\n",
        "data = {\n",
        "    'URL_ID': [],\n",
        "    'POSITIVE SCORE': [],\n",
        "    'NEGATIVE SCORE': [],\n",
        "    'POLARITY SCORE': [],\n",
        "    'SUBJECTIVITY SCORE': []\n",
        "}\n",
        "\n",
        "# Tokenize and calculate scores\n",
        "for file_name in os.listdir(cleaned_files_folder_path):\n",
        "    file_path = os.path.join(cleaned_files_folder_path, file_name)\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase before tokenizing\n",
        "\n",
        "    POSITIVE_SCORE = sum(1 for token in tokens if token in words_dict['positive'])\n",
        "    NEGATIVE_SCORE = sum(1 for token in tokens if token in words_dict['negative'])\n",
        "\n",
        "    POLARITY_SCORE = round((POSITIVE_SCORE - NEGATIVE_SCORE) / ((POSITIVE_SCORE + NEGATIVE_SCORE )+ 0.000001),2) #using round function it rounds the number to two decimal places.\n",
        "    SUBJECTIVITY_SCORE =round( (POSITIVE_SCORE + NEGATIVE_SCORE) / (len(tokens) + 0.000001),2)  #using round function it rounds the number to two decimal places.\n",
        "\n",
        "    # Extract URL ID from the file name (assuming it's the entire name without the extension)\n",
        "    URL_ID = os.path.splitext(file_name)[0]\n",
        "\n",
        "    # Save the data\n",
        "    data['URL_ID'].append(URL_ID)\n",
        "    data['POSITIVE SCORE'].append(POSITIVE_SCORE)\n",
        "    data['NEGATIVE SCORE'].append(NEGATIVE_SCORE)\n",
        "    data['POLARITY SCORE'].append(POLARITY_SCORE)\n",
        "    data['SUBJECTIVITY SCORE'].append(SUBJECTIVITY_SCORE)\n",
        "\n",
        "# Convert the data dictionary into a pandas DataFrame\n",
        "df2 = pd.DataFrame(data)\n",
        "print(df2)\n"
      ],
      "metadata": {
        "id": "r8-GTZ9Iikfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis of Readability**"
      ],
      "metadata": {
        "id": "hTX8pJ90_N0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Downloading required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "\n",
        "d = cmudict.dict()\n",
        "\n",
        "# Helper function to count syllables in a word\n",
        "def nsyl(word):\n",
        "    if word.lower() in d:\n",
        "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
        "    return 0\n",
        "\n",
        "# Function to calculate the Gunning Fog index\n",
        "def gunning_fog(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(words)\n",
        "\n",
        "    if num_words == 0:\n",
        "        return None, None, None  # avoid division by zero\n",
        "\n",
        "    average_sentence_length = round((num_words / num_sentences),2)\n",
        "    complex_words = [word for word in words if nsyl(word) >= 3]\n",
        "    percentage_complex = round(((len(complex_words) / num_words) * 100),2)\n",
        "    fog_index = round((0.4 * (average_sentence_length + percentage_complex)),2)\n",
        "\n",
        "    return average_sentence_length, percentage_complex, fog_index\n",
        "\n",
        "\n",
        "\n",
        "# Data dictionary to store the analysis results\n",
        "data = {\n",
        "    'URL_ID': [],\n",
        "    'AVG SENTENCE LENGTH': [],\n",
        "    'PERCENTAGE OF COMPLEX WORDS': [],\n",
        "    'FOG INDEX': []\n",
        "}\n",
        "\n",
        "# Process each file in the CleanedOutput folder\n",
        "for file_name in os.listdir(cleaned_files_folder_path):\n",
        "    file_path = os.path.join(cleaned_files_folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    avg_sent_len, perc_complex, fog_idx = gunning_fog(text)\n",
        "    if avg_sent_len is not None:  # check to avoid appending when num_words = 0\n",
        "        data['URL_ID'].append(os.path.splitext(file_name)[0])\n",
        "        data['AVG SENTENCE LENGTH'].append(avg_sent_len)\n",
        "        data['PERCENTAGE OF COMPLEX WORDS'].append(perc_complex)\n",
        "        data['FOG INDEX'].append(fog_idx)\n",
        "\n",
        "# Convert data dictionary to a DataFrame\n",
        "df3 = pd.DataFrame(data)\n",
        "print(df3)\n"
      ],
      "metadata": {
        "id": "8NQTulCd8JRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Average Number of Words Per Sentence**"
      ],
      "metadata": {
        "id": "LKX3fYetFyGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Function to calculate the Average Number of Words Per Sentence\n",
        "def avg_words_per_sentence(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(words)\n",
        "\n",
        "    if num_sentences == 0:\n",
        "        return None  # to avoid division by zero\n",
        "\n",
        "    return round((num_words / num_sentences),2)\n",
        "\n",
        "\n",
        "# Data dictionary to store the results\n",
        "data = {\n",
        "    'URL_ID': [],\n",
        "    'AVG NUMBER OF WORDS PER SENTENCE': []\n",
        "}\n",
        "\n",
        "# Process each file in the CleanedOutput folder\n",
        "for file_name in os.listdir(cleaned_files_folder_path):\n",
        "    file_path = os.path.join(cleaned_files_folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    avg_wps = avg_words_per_sentence(text)\n",
        "    if avg_wps is not None:  # to avoid appending when num_sentences = 0\n",
        "        data['URL_ID'].append(os.path.splitext(file_name)[0])\n",
        "        data['AVG NUMBER OF WORDS PER SENTENCE'].append(avg_wps)\n",
        "\n",
        "# Convert data dictionary to a DataFrame\n",
        "df4 = pd.DataFrame(data)\n",
        "print(df4)\n"
      ],
      "metadata": {
        "id": "q0XS0lHBF1H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Complex Word Count**"
      ],
      "metadata": {
        "id": "7LPRKrE5G_p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a dictionary for the syllable count\n",
        "d = cmudict.dict()\n",
        "\n",
        "# Helper function to count syllables in a word\n",
        "def nsyl(word):\n",
        "    if word.lower() in d:\n",
        "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
        "    return 0\n",
        "\n",
        "# Function to count complex words in a text\n",
        "def count_complex_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    complex_words = [word for word in words if nsyl(word) > 2]\n",
        "    return len(complex_words)\n",
        "\n",
        "\n",
        "\n",
        "# Data dictionary to store the results\n",
        "data = {\n",
        "    'URL_ID': [],\n",
        "    'COMPLEX WORD COUNT': []\n",
        "}\n",
        "\n",
        "# Process each file in the CleanedOutput folder\n",
        "for file_name in os.listdir(cleaned_files_folder_path):\n",
        "    file_path = os.path.join(cleaned_files_folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    complex_word_count = count_complex_words(text)\n",
        "    data['URL_ID'].append(os.path.splitext(file_name)[0])\n",
        "    data['COMPLEX WORD COUNT'].append(complex_word_count)\n",
        "\n",
        "# Convert data dictionary to a DataFrame\n",
        "df5 = pd.DataFrame(data)\n",
        "print(df5)\n"
      ],
      "metadata": {
        "id": "tunp80M8HB6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8221d8ad-bd2f-4a42-f039-0eaff5674f70"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      URL_ID  COMPLEX WORD COUNT\n",
            "0        123                 390\n",
            "1        321                 165\n",
            "2       2345                 243\n",
            "3       4321                 295\n",
            "4        432                 295\n",
            "..       ...                 ...\n",
            "109    50921                 122\n",
            "110  51382.8                 279\n",
            "111  51844.6                 343\n",
            "112  52306.4                 263\n",
            "113  52768.2                 208\n",
            "\n",
            "[114 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Count**"
      ],
      "metadata": {
        "id": "K5Ibte6ZOKOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "# Set up stopwords and punctuation filter\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = set('?!.,')\n",
        "\n",
        "# Function to count cleaned words in a text\n",
        "def count_cleaned_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    cleaned_words = [word for word in words if word.lower() not in stop_words and all(char not in punctuations for char in word)]\n",
        "    return len(cleaned_words)\n",
        "\n",
        "\n",
        "\n",
        "# Data dictionary to store the results\n",
        "data = {\n",
        "    'URL_ID': [],\n",
        "    'WORD COUNT': []\n",
        "}\n",
        "\n",
        "# Process each file in the CleanedOutput folder\n",
        "for file_name in os.listdir(cleaned_files_folder_path):\n",
        "    file_path = os.path.join(cleaned_files_folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    cleaned_word_count = count_cleaned_words(text)\n",
        "    data['URL_ID'].append(os.path.splitext(file_name)[0])\n",
        "    data['WORD COUNT'].append(cleaned_word_count)\n",
        "\n",
        "# Convert data dictionary to a DataFrame\n",
        "df6 = pd.DataFrame(data)\n",
        "print(df6)\n"
      ],
      "metadata": {
        "id": "NuaGHGotKoPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Syllable Count Per Word**"
      ],
      "metadata": {
        "id": "IAUlVNliS0w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def count_syllables(word):\n",
        "    vowels = \"aeiouAEIOU\"\n",
        "\n",
        "    # Convert word to lowercase\n",
        "    word = word.lower()\n",
        "\n",
        "    # Count the vowels\n",
        "    count = sum(1 for char in word if char in vowels)\n",
        "\n",
        "    # Adjust for exceptions\n",
        "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
        "        count -= 1\n",
        "\n",
        "    # Ensure we have at least one syllable\n",
        "    return max(1, count)\n",
        "\n",
        "\n",
        "# Data dictionary to store the results\n",
        "data = {\n",
        "    'URL_ID': [],\n",
        "    'Word': [],\n",
        "    'SYLLABLE PER WORD': []\n",
        "}\n",
        "\n",
        "# Process each file in the folder\n",
        "for file_name in os.listdir(cleaned_files_folder_path):\n",
        "    file_path = os.path.join(cleaned_files_folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    for word in words:\n",
        "        syllable_count = count_syllables(word)\n",
        "        data['URL_ID'].append(os.path.splitext(file_name)[0])\n",
        "        data['Word'].append(word)\n",
        "        data['SYLLABLE PER WORD'].append(syllable_count)\n",
        "\n",
        "# Convert data dictionary to a DataFrame\n",
        "df7 = pd.DataFrame(data)\n",
        "print(df7)\n"
      ],
      "metadata": {
        "id": "urX9wCxhOI-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Personal Pronouns**"
      ],
      "metadata": {
        "id": "ozWhcHXhR01e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Define regex pattern for personal pronouns, ignoring case (i.e., both 'I' and 'i' will match)\n",
        "    pattern = r'\\b(?:i|we|my|ours|us)\\b'\n",
        "\n",
        "    # Join words into a single string to match against pattern and handle the 'US' country exception\n",
        "    joined_text = ' '.join([word for word in words if word.lower() != 'us' or word.islower()])\n",
        "\n",
        "    return len(re.findall(pattern, joined_text, flags=re.IGNORECASE))\n",
        "\n",
        "\n",
        "\n",
        "# Data dictionary to store the results\n",
        "data = {\n",
        "    'URL_ID': [],\n",
        "    'PERSONAL PRONOUNS': []\n",
        "}\n",
        "\n",
        "# Process each file in the folder\n",
        "for file_name in os.listdir(cleaned_files_folder_path):\n",
        "    file_path = os.path.join(cleaned_files_folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    pronoun_count = count_personal_pronouns(text)\n",
        "    data['URL_ID'].append(os.path.splitext(file_name)[0])\n",
        "    data['PERSONAL PRONOUNS'].append(pronoun_count)\n",
        "\n",
        "# Convert data dictionary to a DataFrame\n",
        "df8 = pd.DataFrame(data)\n",
        "print(df8)\n"
      ],
      "metadata": {
        "id": "eAdPseYcOI2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Average Word Length**"
      ],
      "metadata": {
        "id": "y6T4-UkPR9hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def average_word_length(text):\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Compute total characters and total words\n",
        "    total_characters = sum(len(word) for word in words)\n",
        "    total_words = len(words)\n",
        "\n",
        "    # To avoid division by zero\n",
        "    if total_words == 0:\n",
        "        return 0\n",
        "\n",
        "    return total_characters / total_words\n",
        "\n",
        "\n",
        "\n",
        "# Data dictionary to store the results\n",
        "data = {\n",
        "    'URL_ID': [],\n",
        "    'AVG WORD LENGTH': []\n",
        "}\n",
        "\n",
        "# Process each file in the folder\n",
        "for file_name in os.listdir(cleaned_files_folder_path):\n",
        "    file_path = os.path.join(cleaned_files_folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    avg_word_len = round(average_word_length(text),2)\n",
        "    data['URL_ID'].append(os.path.splitext(file_name)[0])\n",
        "    data['AVG WORD LENGTH'].append(avg_word_len)\n",
        "\n",
        "# Convert data dictionary to a DataFrame\n",
        "df9 = pd.DataFrame(data)\n",
        "print(df9)\n"
      ],
      "metadata": {
        "id": "i5wmgKJAOIqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Merging All DataFrames In One**"
      ],
      "metadata": {
        "id": "kn31N0CCNWxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes =  [df1, df2, df3, df4, df5, df6, df8, df9] # and so on, or dynamically create this list\n",
        "\n",
        "# Convert the 'URL ID' column of each dataframe to string\n",
        "for df in dataframes:\n",
        "    df['URL_ID'] = df['URL_ID'].astype(str)\n",
        "\n",
        "final_df = dataframes[0]\n",
        "for df in dataframes[1:]:\n",
        "    final_df = final_df.merge(df, on=\"URL_ID\", how=\"inner\")\n",
        "\n",
        "print(final_df)"
      ],
      "metadata": {
        "id": "vlS81ixWGaNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Converting Final Dataframe To Excel And Saving Output In Drive**"
      ],
      "metadata": {
        "id": "zf9pYtM7NeQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/test/FinalOutput.xlsx'\n",
        "\n",
        "# Save the dataframe to Excel\n",
        "final_df.to_excel(file_path, engine='openpyxl', index=False)\n"
      ],
      "metadata": {
        "id": "fRs6R-caLkMv"
      },
      "execution_count": 89,
      "outputs": []
    }
  ]
}